{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\001.txt: \n",
      "Original: 117275\n",
      "Compress: 284840\n",
      "\\002.txt: \n",
      "Original: 6835199\n",
      "Compress: 15820795\n",
      "\\003.txt: \n",
      "Original: 1367999\n",
      "Compress: 5255995\n",
      "\\004.txt: \n",
      "Original: 30988\n",
      "Compress: 118039\n",
      "\\005.txt: \n",
      "Original: 2041506\n",
      "Compress: 2430294\n",
      "\\006.txt: \n",
      "Original: 4079\n",
      "Compress: 15659\n",
      "\\007.txt: \n",
      "Original: 3412\n",
      "Compress: 12990\n",
      "\\008.txt: \n",
      "Original: 7485\n",
      "Compress: 28576\n",
      "\\009.txt: \n",
      "Original: 3772\n",
      "Compress: 14454\n",
      "\\010.txt: \n",
      "Original: 4833\n",
      "Compress: 18397\n",
      "\\011.txt: \n",
      "Original: 2883\n",
      "Compress: 10696\n",
      "\\012.txt: \n",
      "Original: 21985\n",
      "Compress: 85120\n",
      "\\013.txt: \n",
      "Original: 24346\n",
      "Compress: 94380\n",
      "\\014.txt: \n",
      "Original: 55897\n",
      "Compress: 215769\n",
      "\\015.txt: \n",
      "Original: 90390\n",
      "Compress: 349363\n",
      "\\016.txt: \n",
      "Original: 135285\n",
      "Compress: 523070\n",
      "\\017.txt: \n",
      "Original: 221245\n",
      "Compress: 855840\n",
      "\\018.txt: \n",
      "Original: 391595\n",
      "Compress: 1513738\n",
      "\\019.txt: \n",
      "Original: 940745\n",
      "Compress: 3637256\n",
      "\\020.txt: \n",
      "Original: 10556\n",
      "Compress: 39624\n",
      "\\021.txt: \n",
      "Original: 1859258\n",
      "Compress: 7150279\n",
      "\\022.txt: \n",
      "Original: 184186\n",
      "Compress: 707996\n",
      "\\023.txt: \n",
      "Original: 264681\n",
      "Compress: 1020663\n",
      "\\024.txt: \n",
      "Original: 33010\n",
      "Compress: 128834\n",
      "\\025.txt: \n",
      "Original: 788088\n",
      "Compress: 3009728\n",
      "\\026.txt: \n",
      "Original: 710772\n",
      "Compress: 2517859\n",
      "\\027.txt: \n",
      "Original: 230593\n",
      "Compress: 886687\n",
      "\\028.txt: \n",
      "Original: 241315\n",
      "Compress: 950173\n",
      "\\029.txt: \n",
      "Original: 4422549\n",
      "Compress: 16407764\n",
      "\\030.txt: \n",
      "Original: 1887519\n",
      "Compress: 7383973\n",
      "\\031.txt: \n",
      "Original: 4152594\n",
      "Compress: 16025025\n",
      "\\032.txt: \n",
      "Original: 7556130\n",
      "Compress: 29446814\n",
      "\\033.txt: \n",
      "Original: 7414259\n",
      "Compress: 29180781\n",
      "\\034.txt: \n",
      "Original: 5290128\n",
      "Compress: 20813551\n",
      "\\035.txt: \n",
      "Original: 5441763\n",
      "Compress: 21409931\n",
      "\\036.txt: \n",
      "Original: 5374895\n",
      "Compress: 21133166\n",
      "\\037.txt: \n",
      "Original: 5003090\n",
      "Compress: 19685117\n",
      "\\038.txt: \n",
      "Original: 110592\n",
      "Compress: 238983\n",
      "\\039.txt: \n",
      "Original: 388080\n",
      "Compress: 63514\n",
      "\\040.txt: \n",
      "Original: 359952\n",
      "Compress: 1247769\n"
     ]
    }
   ],
   "source": [
    "#Run Length Coding\n",
    "import os\n",
    "def compress(input_string):\n",
    "    count = 1\n",
    "    prev = ''\n",
    "    lst = []\n",
    "    for character in input_string:\n",
    "        if character != prev:\n",
    "            if prev:\n",
    "                entry = (prev,count)\n",
    "                lst.append(entry)\n",
    "                count = 1\n",
    "            prev = character\n",
    "        else:\n",
    "            count += 1\n",
    "    else:\n",
    "        entry = (character,count)\n",
    "        lst.append(entry)\n",
    "    return lst\n",
    " \n",
    " \n",
    "def decompress(lst):\n",
    "    q = \"\"\n",
    "    for character, count in lst:\n",
    "        q += character * count\n",
    "    return q\n",
    " \n",
    "def save_to_file(path,thelist):\n",
    "    with open(path,\"wt\") as thefile:\n",
    "        for item in thelist:\n",
    "            for el in item:\n",
    "                if el == \"\\n\":\n",
    "                    thefile.write(\"\\\\n`\")\n",
    "                elif el==\"`\":\n",
    "                    thefile.write(\"\\\"`\")\n",
    "                else:\n",
    "                    thefile.write(\"%s`\" % el)\n",
    "        thefile.seek(-1, os.SEEK_END)\n",
    "        thefile.truncate()\n",
    "        thefile.close()\n",
    "    return\n",
    "\n",
    "def read_from_file(path):\n",
    "    with open(path, \"r\") as ins:\n",
    "        array = []\n",
    "        t_array=[]\n",
    "        check_n=0\n",
    "        flag=0\n",
    "        i=0\n",
    "        for line in ins.read().split('`'):\n",
    "            if i == 0:\n",
    "                if line==\"\\\\n\":\n",
    "                    t_array.append(\"\\n\")\n",
    "                else:\n",
    "                    t_array.append(str(line))\n",
    "                i=i+1\n",
    "            elif i == 1:\n",
    "                t_array.append(int(line))\n",
    "                i=i+1\n",
    "            if i == 2:\n",
    "                array.append((t_array[flag],t_array[flag+1]))\n",
    "                flag=flag+2\n",
    "                i=0\n",
    "        ins.close()      \n",
    "    return array\n",
    "\n",
    "def runRLC(self):\n",
    "    path = os.getcwd()+self\n",
    "    result_file = open(path+\"\\\\result\\\\resultRLC.txt\",\"wt\")\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(\"\\\\\"+filename+\": \")\n",
    "            test_file = open(path+\"\\\\\"+filename,\"r\")\n",
    "            write_file = path+\"\\compress\\compressRLC_\"+filename\n",
    "            \n",
    "            text = test_file.read()\n",
    "            a=compress(text)\n",
    "            save_to_file(write_file,a)\n",
    "            c=read_from_file(write_file)\n",
    "            b=decompress(c)\n",
    "            \n",
    "            save_file = path+\"\\decompress\\decompressRLC_\"+filename\n",
    "            decompress_file = open(save_file,\"wt\")\n",
    "            decompress_file.write(b)\n",
    "            decompress_file.close()\n",
    "            test_file.close()\n",
    "            compress_size=os.path.getsize(write_file)\n",
    "            decompress_size=os.path.getsize(save_file)\n",
    "            \n",
    "            print(\"Original: %d\"%decompress_size)\n",
    "            print(\"Compress: %d\"%compress_size)\n",
    "            result_file.write(str(filename)+\"\\t\"+str(decompress_size)+\"\\t\"+str(compress_size)+\"\\n\")\n",
    "            \n",
    "    result_file.close()\n",
    "    return \n",
    "\n",
    "\n",
    "self = \"\\dataset\"\n",
    "runRLC(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\000.txt: \n",
      "asdasd asd asd asd asd asd asd asd a\n",
      "qwevv    qwe qwe \t\t qwe qwe qwe \n",
      "\n",
      "asd\n",
      "       asdadsadasd\n",
      "\t\t\t\t\tdsadsad\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "das\n",
      "Original: 128\n",
      "Compress: 223\n",
      "\\001.txt: \n",
      "111111111111\n",
      "1111\n",
      "111\n",
      "1111\n",
      "0\t000000000\n",
      "\n",
      ",\t\t,\t ,,,,,,q\t\tw\teqeqwewqeqwe,,,dsadsdas,\n",
      "00000000 \n",
      "sdasd''\n",
      "\t\n",
      "\t\t'''wqeqweqw eq we'qw 'eq'we 'qwe\n",
      "Original: 146\n",
      "Compress: 287\n",
      "\\004.txt: \n",
      "This document describes the contents of the MSR Abstractive Text Compression Dataset Release\n",
      "\n",
      "This is the dataset described in the paper \"A dataset and evaluation metrics for abstractive sentence and paragraph compression\" [1].\n",
      "\n",
      "The release contains the following data:\n",
      "\n",
      "1. Original sentences and short paragraphs (texts) with corresponding crowd-sourced compressed versions and crowd-sourced ratings of each versions\n",
      "\n",
      "The directory RawData contains the texts in the training, validation, and test sets, with file names  train.tsv , valid.tsv, and test.tsv\n",
      "\n",
      "The format of each file is as follows:\n",
      "\n",
      "On each line, we have information for one source text and corresponding compressed versions.\n",
      "\n",
      "The format is SourceInfo[ ||| CompressionInfo]+\n",
      "\n",
      "SourceInfo is information on the soure text and has the following fields (tab-separated):\n",
      "\n",
      "SourceID \\t Domain \\t SourceText\n",
      "\n",
      "The SourceID has one or more integers connected with _, for exmaple \"15\" or \"101_102\". There is one integer per sentence in the source text.\n",
      "\n",
      "CompressionInfo is information about a compression for the SourceText.\n",
      "\n",
      "CompressionInfo has the following fields [tab-separated]:\n",
      "\n",
      "CompressedText \\t JudgeId \\t numRatings[\\t Rating]^numRatings\n",
      "\n",
      "This is the compressed text (un-tokenized), the JudgeId (the anonymized ids of one or more crowd-workers that proposed this compression ), and indication of how many ratings we have and the sequence of ratings.\n",
      "\n",
      "Each Rating has the format:\n",
      "\n",
      "CombinedRatingValue \\t Meaning_quality_string \\t Grammar_quality_string\n",
      "\n",
      "The CombinedRatingValue is sufficient to indicate the meaning and grammaticality qualitiy values assigned by a single rater, and the subseqeunt string values are not strictly necessary.\n",
      "\n",
      "The different numeric values of CombinedRatingValue have the following meanings:\n",
      "\n",
      "6\tMost important meaning Flawless language      (3 on meaning and 3 on grammar as per the paper's terminology)\n",
      "7\tMost important meaning Minor errors           (3 on meaning and 2 on grammar)\n",
      "9\tMost important meaning Disfluent or incomprehensible (3 on meaning and 1 on grammar)\n",
      "11\tMuch meaning Flawless language                (2 on meaning and 3 on grammar)\n",
      "12\tMuch meaning Minor errors                     (2 on meaning and 2 on grammar)\n",
      "14\tMuch meaning Disfluent or incomprehensible    (2 on meaning and 1 on grammar)\n",
      "21\tLittle or none meaning Flawless language      (1 on meaning and 3 on grammar)\n",
      "22\tLittle or none meaning Minor errors           (1 on meaning and 2 on grammar)\n",
      "24\tLittle or none meaning Disfluent or incomprehensible (1 on meaning and 1 on grammar)\n",
      "\n",
      "2. Processed original texts with corresponding crowd-sourced compressions\n",
      "\n",
      "For convenince, we also include processed versions of the texts in 1 above, where processing includes sentence-breaking, tokenization, dependency and constituency parsing, and word alignment from compressed to original texts.\n",
      "\n",
      "The files in Processed\\train.tsv Processed\\valid.tsv and Processed\\test.tsv contain information corresponding to the data in RawData\n",
      "\n",
      "The format of the processed files is as follows:\n",
      "\n",
      "On every line, we have:\n",
      "\n",
      "ProccessedSourceInfo[ ||| ProcessedCompressionInfo]+\n",
      "\n",
      "The order of the lines and the order of the compressions for each source is the same as in the rawdata.\n",
      "\n",
      "ProccessedSourceInfo contains the following tab-separated fields: \n",
      "\n",
      "SourceID \\t TokenizedSourceText \\t SourceDependencyTrees \\t SourceConstituencyTrees\n",
      "\n",
      "The SourceIDs align with the SourceIDs in the RawData\n",
      "TokenizedSourceText has been obtained using the tokenizer from the Stanford CoreNLP library. Sentence boundaries are indicated via <eos> tokens.\n",
      "SourceDependencyTrees are obtained using the Stanford parser version 3.4.1, and constituency trees are obtained using the same parser. In case of multiple sentences in TokenizedSourceText, there will be mutliple dependency and consttituency trees separated by <eos>\n",
      "\n",
      "ProcessedCompressionInfo contains the following tab-separated fields:\n",
      "\n",
      "TokenizedCompressedText \\t CompressionDependencyTrees \\t CompressionConstituencyTrees \\t WordAlignment\n",
      "\n",
      "the processed compressions appear in the same order as the original un-processed compressions in the RawData\n",
      "The Stanford tokenizer and parsers have been used here as well, and <eos> is used as a sentence separator.\n",
      "\n",
      "The WordAlignment indicates a single source text token corresponding to each compression token. Thsi was obtained using Jacana and post-processing to align the null-aligned tokens.\n",
      "\n",
      "3. Editting history for the generation of the text compressions.\n",
      "\n",
      "As mentioned in the paper, we collected the history of edits that a crowd worker performed to create the compression. Some workers copied and pasted the original text and performed deletions. Others typed the compressed text from scratch. We recorded the status of the output field on every click, paste, or type action. \n",
      "\n",
      "This information is available in the file compressionhistory.tsv\n",
      "\n",
      "The format of the file is as follows (tab-separated):\n",
      "\n",
      "sourceID \\t Domain \\t SourceText \\t CompressedText \\t judgeID \\t AverageMeaningQuality \\t AverageGrammarQuality \\t TimespentOnTask \\t EditHistory\n",
      "\n",
      "Here the sourceIDs are aligned to the ones used in 1. and 2. above. The Source and Compressed texts are un-processed (not tokenized). The judgeID is the anonymized id of the crowd worker that performed the task (these ids were also used in 1.). The average quality is obtained by averaging the numerical values of mtuliple rating judgements on meaning preservation and grammaticality.\n",
      "\n",
      "The edit history shows a sequence of snapshots of the output text field after each action. The different snapshots are separated by a sequence of two spaces, so the delimiting is not entirely unambiguous.\n",
      "\n",
      "\n",
      "4. The gudelines provided to the crowd workers are available in the Documents direcory. A screenshot of the interface can be seen there as well. There were separate gudelines for shortening a single sentence versus a short paragraph. The rating gudelines were the same for all rating tasks.\n",
      "\n",
      "5. We also provide the output of the four automatic compression systems included in the study in [1], together with quality judgements of their meaning and grammaticality.\n",
      "\n",
      "The format of the files is the same as the format of the crowd compressions in 1., except that the judgeIds are now the ids of the systems. We provide the four system outputs in separate files.\n",
      "\n",
      "The four systems with corresponding rated output files, are, under the sub-directory RawData:\n",
      " \n",
      " test_output.t3.tsv is from the T3 system described in [2] and available from the first authors' website.\n",
      " test_outut.ilp.tsv is from the ILP model of [3]  with implementation avaialble at [4].\n",
      " test_output.seq2seq.tsv is from our re-implementation of [5].\n",
      " test_output.namas.tsv is from the open-source implementation [6] of [7]. \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " References\n",
      " \n",
      " 1. Kristina Toutanova, Chris Brockett, Ke M. Tran, and Saleema Amershi. A dataset and evaluation metrics for abstractive sentence and paragraph compression. In Proceedings of EMNLP 2016.\n",
      " 2. Trevor Cohn and Mirella Lapata. Sentence compression beyond word deletion. In Proceedings of COLING 2008.\n",
      " 3. James Clarke and Mirella Lapata. Global inference for sentence compression: An integer linear programming approach. JAIR 2008.\n",
      " 4. https://github.com/cnap/sentence-compression\n",
      " 5. Katja Filippova, Enrique Alfonseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals. Sentence  compression  by  deletion  with  LSTMs. In Proceedings of EMNLP 2015.\n",
      " 6. https://github.com/facebook/NAMAS\n",
      " 7. Alexander M. Rush, Sumit Chopra, and  Jason  Weston.A  neural attention model for abstractive sentence summarization. In Proceedings of EMNLP 2015.\n",
      " \n",
      " \n",
      "\n",
      "This document describes the contents of the MSR Abstractive Text Compression Dataset Release\n",
      "\n",
      "This is the dataset described in the paper \"A dataset and evaluation metrics for abstractive sentence and paragraph compression\" [1].\n",
      "\n",
      "The release contains the following data:\n",
      "\n",
      "1. Original sentences and short paragraphs (texts) with corresponding crowd-sourced compressed versions and crowd-sourced ratings of each versions\n",
      "\n",
      "The directory RawData contains the texts in the training, validation, and test sets, with file names  train.tsv , valid.tsv, and test.tsv\n",
      "\n",
      "The format of each file is as follows:\n",
      "\n",
      "On each line, we have information for one source text and corresponding compressed versions.\n",
      "\n",
      "The format is SourceInfo[ ||| CompressionInfo]+\n",
      "\n",
      "SourceInfo is information on the soure text and has the following fields (tab-separated):\n",
      "\n",
      "SourceID \\t Domain \\t SourceText\n",
      "\n",
      "The SourceID has one or more integers connected with _, for exmaple \"15\" or \"101_102\". There is one integer per sentence in the source text.\n",
      "\n",
      "CompressionInfo is information about a compression for the SourceText.\n",
      "\n",
      "CompressionInfo has the following fields [tab-separated]:\n",
      "\n",
      "CompressedText \\t JudgeId \\t numRatings[\\t Rating]^numRatings\n",
      "\n",
      "This is the compressed text (un-tokenized), the JudgeId (the anonymized ids of one or more crowd-workers that proposed this compression ), and indication of how many ratings we have and the sequence of ratings.\n",
      "\n",
      "Each Rating has the format:\n",
      "\n",
      "CombinedRatingValue \\t Meaning_quality_string \\t Grammar_quality_string\n",
      "\n",
      "The CombinedRatingValue is sufficient to indicate the meaning and grammaticality qualitiy values assigned by a single rater, and the subseqeunt string values are not strictly necessary.\n",
      "\n",
      "The different numeric values of CombinedRatingValue have the following meanings:\n",
      "\n",
      "6\tMost important meaning Flawless language      (3 on meaning and 3 on grammar as per the paper's terminology)\n",
      "7\tMost important meaning Minor errors           (3 on meaning and 2 on grammar)\n",
      "9\tMost important meaning Disfluent or incomprehensible (3 on meaning and 1 on grammar)\n",
      "11\tMuch meaning Flawless language                (2 on meaning and 3 on grammar)\n",
      "12\tMuch meaning Minor errors                     (2 on meaning and 2 on grammar)\n",
      "14\tMuch meaning Disfluent or incomprehensible    (2 on meaning and 1 on grammar)\n",
      "21\tLittle or none meaning Flawless language      (1 on meaning and 3 on grammar)\n",
      "22\tLittle or none meaning Minor errors           (1 on meaning and 2 on grammar)\n",
      "24\tLittle or none meaning Disfluent or incomprehensible (1 on meaning and 1 on grammar)\n",
      "\n",
      "2. Processed original texts with corresponding crowd-sourced compressions\n",
      "\n",
      "For convenince, we also include processed versions of the texts in 1 above, where processing includes sentence-breaking, tokenization, dependency and constituency parsing, and word alignment from compressed to original texts.\n",
      "\n",
      "The files in Processed\\train.tsv Processed\\valid.tsv and Processed\\test.tsv contain information corresponding to the data in RawData\n",
      "\n",
      "The format of the processed files is as follows:\n",
      "\n",
      "On every line, we have:\n",
      "\n",
      "ProccessedSourceInfo[ ||| ProcessedCompressionInfo]+\n",
      "\n",
      "The order of the lines and the order of the compressions for each source is the same as in the rawdata.\n",
      "\n",
      "ProccessedSourceInfo contains the following tab-separated fields: \n",
      "\n",
      "SourceID \\t TokenizedSourceText \\t SourceDependencyTrees \\t SourceConstituencyTrees\n",
      "\n",
      "The SourceIDs align with the SourceIDs in the RawData\n",
      "TokenizedSourceText has been obtained using the tokenizer from the Stanford CoreNLP library. Sentence boundaries are indicated via <eos> tokens.\n",
      "SourceDependencyTrees are obtained using the Stanford parser version 3.4.1, and constituency trees are obtained using the same parser. In case of multiple sentences in TokenizedSourceText, there will be mutliple dependency and consttituency trees separated by <eos>\n",
      "\n",
      "ProcessedCompressionInfo contains the following tab-separated fields:\n",
      "\n",
      "TokenizedCompressedText \\t CompressionDependencyTrees \\t CompressionConstituencyTrees \\t WordAlignment\n",
      "\n",
      "the processed compressions appear in the same order as the original un-processed compressions in the RawData\n",
      "The Stanford tokenizer and parsers have been used here as well, and <eos> is used as a sentence separator.\n",
      "\n",
      "The WordAlignment indicates a single source text token corresponding to each compression token. Thsi was obtained using Jacana and post-processing to align the null-aligned tokens.\n",
      "\n",
      "3. Editting history for the generation of the text compressions.\n",
      "\n",
      "As mentioned in the paper, we collected the history of edits that a crowd worker performed to create the compression. Some workers copied and pasted the original text and performed deletions. Others typed the compressed text from scratch. We recorded the status of the output field on every click, paste, or type action. \n",
      "\n",
      "This information is available in the file compressionhistory.tsv\n",
      "\n",
      "The format of the file is as follows (tab-separated):\n",
      "\n",
      "sourceID \\t Domain \\t SourceText \\t CompressedText \\t judgeID \\t AverageMeaningQuality \\t AverageGrammarQuality \\t TimespentOnTask \\t EditHistory\n",
      "\n",
      "Here the sourceIDs are aligned to the ones used in 1. and 2. above. The Source and Compressed texts are un-processed (not tokenized). The judgeID is the anonymized id of the crowd worker that performed the task (these ids were also used in 1.). The average quality is obtained by averaging the numerical values of mtuliple rating judgements on meaning preservation and grammaticality.\n",
      "\n",
      "The edit history shows a sequence of snapshots of the output text field after each action. The different snapshots are separated by a sequence of two spaces, so the delimiting is not entirely unambiguous.\n",
      "\n",
      "\n",
      "4. The gudelines provided to the crowd workers are available in the Documents direcory. A screenshot of the interface can be seen there as well. There were separate gudelines for shortening a single sentence versus a short paragraph. The rating gudelines were the same for all rating tasks.\n",
      "\n",
      "5. We also provide the output of the four automatic compression systems included in the study in [1], together with quality judgements of their meaning and grammaticality.\n",
      "\n",
      "The format of the files is the same as the format of the crowd compressions in 1., except that the judgeIds are now the ids of the systems. We provide the four system outputs in separate files.\n",
      "\n",
      "The four systems with corresponding rated output files, are, under the sub-directory RawData:\n",
      " \n",
      " test_output.t3.tsv is from the T3 system described in [2] and available from the first authors' website.\n",
      " test_outut.ilp.tsv is from the ILP model of [3]  with implementation avaialble at [4].\n",
      " test_output.seq2seq.tsv is from our re-implementation of [5].\n",
      " test_output.namas.tsv is from the open-source implementation [6] of [7]. \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " References\n",
      " \n",
      " 1. Kristina Toutanova, Chris Brockett, Ke M. Tran, and Saleema Amershi. A dataset and evaluation metrics for abstractive sentence and paragraph compression. In Proceedings of EMNLP 2016.\n",
      " 2. Trevor Cohn and Mirella Lapata. Sentence compression beyond word deletion. In Proceedings of COLING 2008.\n",
      " 3. James Clarke and Mirella Lapata. Global inference for sentence compression: An integer linear programming approach. JAIR 2008.\n",
      " 4. https://github.com/cnap/sentence-compression\n",
      " 5. Katja Filippova, Enrique Alfonseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals. Sentence  compression  by  deletion  with  LSTMs. In Proceedings of EMNLP 2015.\n",
      " 6. https://github.com/facebook/NAMAS\n",
      " 7. Alexander M. Rush, Sumit Chopra, and  Jason  Weston.A  neural attention model for abstractive sentence summarization. In Proceedings of EMNLP 2015.\n",
      " \n",
      " \n",
      "\n",
      "This document describes the contents of the MSR Abstractive Text Compression Dataset Release\n",
      "\n",
      "This is the dataset described in the paper \"A dataset and evaluation metrics for abstractive sentence and paragraph compression\" [1].\n",
      "\n",
      "The release contains the following data:\n",
      "\n",
      "1. Original sentences and short paragraphs (texts) with corresponding crowd-sourced compressed versions and crowd-sourced ratings of each versions\n",
      "\n",
      "The directory RawData contains the texts in the training, validation, and test sets, with file names  train.tsv , valid.tsv, and test.tsv\n",
      "\n",
      "The format of each file is as follows:\n",
      "\n",
      "On each line, we have information for one source text and corresponding compressed versions.\n",
      "\n",
      "The format is SourceInfo[ ||| CompressionInfo]+\n",
      "\n",
      "SourceInfo is information on the soure text and has the following fields (tab-separated):\n",
      "\n",
      "SourceID \\t Domain \\t SourceText\n",
      "\n",
      "The SourceID has one or more integers connected with _, for exmaple \"15\" or \"101_102\". There is one integer per sentence in the source text.\n",
      "\n",
      "CompressionInfo is information about a compression for the SourceText.\n",
      "\n",
      "CompressionInfo has the following fields [tab-separated]:\n",
      "\n",
      "CompressedText \\t JudgeId \\t numRatings[\\t Rating]^numRatings\n",
      "\n",
      "This is the compressed text (un-tokenized), the JudgeId (the anonymized ids of one or more crowd-workers that proposed this compression ), and indication of how many ratings we have and the sequence of ratings.\n",
      "\n",
      "Each Rating has the format:\n",
      "\n",
      "CombinedRatingValue \\t Meaning_quality_string \\t Grammar_quality_string\n",
      "\n",
      "The CombinedRatingValue is sufficient to indicate the meaning and grammaticality qualitiy values assigned by a single rater, and the subseqeunt string values are not strictly necessary.\n",
      "\n",
      "The different numeric values of CombinedRatingValue have the following meanings:\n",
      "\n",
      "6\tMost important meaning Flawless language      (3 on meaning and 3 on grammar as per the paper's terminology)\n",
      "7\tMost important meaning Minor errors           (3 on meaning and 2 on grammar)\n",
      "9\tMost important meaning Disfluent or incomprehensible (3 on meaning and 1 on grammar)\n",
      "11\tMuch meaning Flawless language                (2 on meaning and 3 on grammar)\n",
      "12\tMuch meaning Minor errors                     (2 on meaning and 2 on grammar)\n",
      "14\tMuch meaning Disfluent or incomprehensible    (2 on meaning and 1 on grammar)\n",
      "21\tLittle or none meaning Flawless language      (1 on meaning and 3 on grammar)\n",
      "22\tLittle or none meaning Minor errors           (1 on meaning and 2 on grammar)\n",
      "24\tLittle or none meaning Disfluent or incomprehensible (1 on meaning and 1 on grammar)\n",
      "\n",
      "2. Processed original texts with corresponding crowd-sourced compressions\n",
      "\n",
      "For convenince, we also include processed versions of the texts in 1 above, where processing includes sentence-breaking, tokenization, dependency and constituency parsing, and word alignment from compressed to original texts.\n",
      "\n",
      "The files in Processed\\train.tsv Processed\\valid.tsv and Processed\\test.tsv contain information corresponding to the data in RawData\n",
      "\n",
      "The format of the processed files is as follows:\n",
      "\n",
      "On every line, we have:\n",
      "\n",
      "ProccessedSourceInfo[ ||| ProcessedCompressionInfo]+\n",
      "\n",
      "The order of the lines and the order of the compressions for each source is the same as in the rawdata.\n",
      "\n",
      "ProccessedSourceInfo contains the following tab-separated fields: \n",
      "\n",
      "SourceID \\t TokenizedSourceText \\t SourceDependencyTrees \\t SourceConstituencyTrees\n",
      "\n",
      "The SourceIDs align with the SourceIDs in the RawData\n",
      "TokenizedSourceText has been obtained using the tokenizer from the Stanford CoreNLP library. Sentence boundaries are indicated via <eos> tokens.\n",
      "SourceDependencyTrees are obtained using the Stanford parser version 3.4.1, and constituency trees are obtained using the same parser. In case of multiple sentences in TokenizedSourceText, there will be mutliple dependency and consttituency trees separated by <eos>\n",
      "\n",
      "ProcessedCompressionInfo contains the following tab-separated fields:\n",
      "\n",
      "TokenizedCompressedText \\t CompressionDependencyTrees \\t CompressionConstituencyTrees \\t WordAlignment\n",
      "\n",
      "the processed compressions appear in the same order as the original un-processed compressions in the RawData\n",
      "The Stanford tokenizer and parsers have been used here as well, and <eos> is used as a sentence separator.\n",
      "\n",
      "The WordAlignment indicates a single source text token corresponding to each compression token. Thsi was obtained using Jacana and post-processing to align the null-aligned tokens.\n",
      "\n",
      "3. Editting history for the generation of the text compressions.\n",
      "\n",
      "As mentioned in the paper, we collected the history of edits that a crowd worker performed to create the compression. Some workers copied and pasted the original text and performed deletions. Others typed the compressed text from scratch. We recorded the status of the output field on every click, paste, or type action. \n",
      "\n",
      "This information is available in the file compressionhistory.tsv\n",
      "\n",
      "The format of the file is as follows (tab-separated):\n",
      "\n",
      "sourceID \\t Domain \\t SourceText \\t CompressedText \\t judgeID \\t AverageMeaningQuality \\t AverageGrammarQuality \\t TimespentOnTask \\t EditHistory\n",
      "\n",
      "Here the sourceIDs are aligned to the ones used in 1. and 2. above. The Source and Compressed texts are un-processed (not tokenized). The judgeID is the anonymized id of the crowd worker that performed the task (these ids were also used in 1.). The average quality is obtained by averaging the numerical values of mtuliple rating judgements on meaning preservation and grammaticality.\n",
      "\n",
      "The edit history shows a sequence of snapshots of the output text field after each action. The different snapshots are separated by a sequence of two spaces, so the delimiting is not entirely unambiguous.\n",
      "\n",
      "\n",
      "4. The gudelines provided to the crowd workers are available in the Documents direcory. A screenshot of the interface can be seen there as well. There were separate gudelines for shortening a single sentence versus a short paragraph. The rating gudelines were the same for all rating tasks.\n",
      "\n",
      "5. We also provide the output of the four automatic compression systems included in the study in [1], together with quality judgements of their meaning and grammaticality.\n",
      "\n",
      "The format of the files is the same as the format of the crowd compressions in 1., except that the judgeIds are now the ids of the systems. We provide the four system outputs in separate files.\n",
      "\n",
      "The four systems with corresponding rated output files, are, under the sub-directory RawData:\n",
      " \n",
      " test_output.t3.tsv is from the T3 system described in [2] and available from the first authors' website.\n",
      " test_outut.ilp.tsv is from the ILP model of [3]  with implementation avaialble at [4].\n",
      " test_output.seq2seq.tsv is from our re-implementation of [5].\n",
      " test_output.namas.tsv is from the open-source implementation [6] of [7]. \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " References\n",
      " \n",
      " 1. Kristina Toutanova, Chris Brockett, Ke M. Tran, and Saleema Amershi. A dataset and evaluation metrics for abstractive sentence and paragraph compression. In Proceedings of EMNLP 2016.\n",
      " 2. Trevor Cohn and Mirella Lapata. Sentence compression beyond word deletion. In Proceedings of COLING 2008.\n",
      " 3. James Clarke and Mirella Lapata. Global inference for sentence compression: An integer linear programming approach. JAIR 2008.\n",
      " 4. https://github.com/cnap/sentence-compression\n",
      " 5. Katja Filippova, Enrique Alfonseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals. Sentence  compression  by  deletion  with  LSTMs. In Proceedings of EMNLP 2015.\n",
      " 6. https://github.com/facebook/NAMAS\n",
      " 7. Alexander M. Rush, Sumit Chopra, and  Jason  Weston.A  neural attention model for abstractive sentence summarization. In Proceedings of EMNLP 2015.\n",
      " \n",
      " \n",
      "This document describes the contents of the MSR Abstractive Text Compression Dataset Release\n",
      "\n",
      "This is the dataset described in the paper \"A dataset and evaluation metrics for abstractive sentence and paragraph compression\" [1].\n",
      "\n",
      "The release contains the following data:\n",
      "\n",
      "1. Original sentences and short paragraphs (texts) with corresponding crowd-sourced compressed versions and crowd-sourced ratings of each versions\n",
      "\n",
      "The directory RawData contains the texts in the training, validation, and test sets, with file names  train.tsv , valid.tsv, and test.tsv\n",
      "\n",
      "The format of each file is as follows:\n",
      "\n",
      "On each line, we have information for one source text and corresponding compressed versions.\n",
      "\n",
      "The format is SourceInfo[ ||| CompressionInfo]+\n",
      "\n",
      "SourceInfo is information on the soure text and has the following fields (tab-separated):\n",
      "\n",
      "SourceID \\t Domain \\t SourceText\n",
      "\n",
      "The SourceID has one or more integers connected with _, for exmaple \"15\" or \"101_102\". There is one integer per sentence in the source text.\n",
      "\n",
      "CompressionInfo is information about a compression for the SourceText.\n",
      "\n",
      "CompressionInfo has the following fields [tab-separated]:\n",
      "\n",
      "CompressedText \\t JudgeId \\t numRatings[\\t Rating]^numRatings\n",
      "\n",
      "This is the compressed text (un-tokenized), the JudgeId (the anonymized ids of one or more crowd-workers that proposed this compression ), and indication of how many ratings we have and the sequence of ratings.\n",
      "\n",
      "Each Rating has the format:\n",
      "\n",
      "CombinedRatingValue \\t Meaning_quality_string \\t Grammar_quality_string\n",
      "\n",
      "The CombinedRatingValue is sufficient to indicate the meaning and grammaticality qualitiy values assigned by a single rater, and the subseqeunt string values are not strictly necessary.\n",
      "\n",
      "The different numeric values of CombinedRatingValue have the following meanings:\n",
      "\n",
      "6\tMost important meaning Flawless language      (3 on meaning and 3 on grammar as per the paper's terminology)\n",
      "7\tMost important meaning Minor errors           (3 on meaning and 2 on grammar)\n",
      "9\tMost important meaning Disfluent or incomprehensible (3 on meaning and 1 on grammar)\n",
      "11\tMuch meaning Flawless language                (2 on meaning and 3 on grammar)\n",
      "12\tMuch meaning Minor errors                     (2 on meaning and 2 on grammar)\n",
      "14\tMuch meaning Disfluent or incomprehensible    (2 on meaning and 1 on grammar)\n",
      "21\tLittle or none meaning Flawless language      (1 on meaning and 3 on grammar)\n",
      "22\tLittle or none meaning Minor errors           (1 on meaning and 2 on grammar)\n",
      "24\tLittle or none meaning Disfluent or incomprehensible (1 on meaning and 1 on grammar)\n",
      "\n",
      "2. Processed original texts with corresponding crowd-sourced compressions\n",
      "\n",
      "For convenince, we also include processed versions of the texts in 1 above, where processing includes sentence-breaking, tokenization, dependency and constituency parsing, and word alignment from compressed to original texts.\n",
      "\n",
      "The files in Processed\\train.tsv Processed\\valid.tsv and Processed\\test.tsv contain information corresponding to the data in RawData\n",
      "\n",
      "The format of the processed files is as follows:\n",
      "\n",
      "On every line, we have:\n",
      "\n",
      "ProccessedSourceInfo[ ||| ProcessedCompressionInfo]+\n",
      "\n",
      "The order of the lines and the order of the compressions for each source is the same as in the rawdata.\n",
      "\n",
      "ProccessedSourceInfo contains the following tab-separated fields: \n",
      "\n",
      "SourceID \\t TokenizedSourceText \\t SourceDependencyTrees \\t SourceConstituencyTrees\n",
      "\n",
      "The SourceIDs align with the SourceIDs in the RawData\n",
      "TokenizedSourceText has been obtained using the tokenizer from the Stanford CoreNLP library. Sentence boundaries are indicated via <eos> tokens.\n",
      "SourceDependencyTrees are obtained using the Stanford parser version 3.4.1, and constituency trees are obtained using the same parser. In case of multiple sentences in TokenizedSourceText, there will be mutliple dependency and consttituency trees separated by <eos>\n",
      "\n",
      "ProcessedCompressionInfo contains the following tab-separated fields:\n",
      "\n",
      "TokenizedCompressedText \\t CompressionDependencyTrees \\t CompressionConstituencyTrees \\t WordAlignment\n",
      "\n",
      "the processed compressions appear in the same order as the original un-processed compressions in the RawData\n",
      "The Stanford tokenizer and parsers have been used here as well, and <eos> is used as a sentence separator.\n",
      "\n",
      "The WordAlignment indicates a single source text token corresponding to each compression token. Thsi was obtained using Jacana and post-processing to align the null-aligned tokens.\n",
      "\n",
      "3. Editting history for the generation of the text compressions.\n",
      "\n",
      "As mentioned in the paper, we collected the history of edits that a crowd worker performed to create the compression. Some workers copied and pasted the original text and performed deletions. Others typed the compressed text from scratch. We recorded the status of the output field on every click, paste, or type action. \n",
      "\n",
      "This information is available in the file compressionhistory.tsv\n",
      "\n",
      "The format of the file is as follows (tab-separated):\n",
      "\n",
      "sourceID \\t Domain \\t SourceText \\t CompressedText \\t judgeID \\t AverageMeaningQuality \\t AverageGrammarQuality \\t TimespentOnTask \\t EditHistory\n",
      "\n",
      "Here the sourceIDs are aligned to the ones used in 1. and 2. above. The Source and Compressed texts are un-processed (not tokenized). The judgeID is the anonymized id of the crowd worker that performed the task (these ids were also used in 1.). The average quality is obtained by averaging the numerical values of mtuliple rating judgements on meaning preservation and grammaticality.\n",
      "\n",
      "The edit history shows a sequence of snapshots of the output text field after each action. The different snapshots are separated by a sequence of two spaces, so the delimiting is not entirely unambiguous.\n",
      "\n",
      "\n",
      "4. The gudelines provided to the crowd workers are available in the Documents direcory. A screenshot of the interface can be seen there as well. There were separate gudelines for shortening a single sentence versus a short paragraph. The rating gudelines were the same for all rating tasks.\n",
      "\n",
      "5. We also provide the output of the four automatic compression systems included in the study in [1], together with quality judgements of their meaning and grammaticality.\n",
      "\n",
      "The format of the files is the same as the format of the crowd compressions in 1., except that the judgeIds are now the ids of the systems. We provide the four system outputs in separate files.\n",
      "\n",
      "The four systems with corresponding rated output files, are, under the sub-directory RawData:\n",
      " \n",
      " test_output.t3.tsv is from the T3 system described in [2] and available from the first authors' website.\n",
      " test_outut.ilp.tsv is from the ILP model of [3]  with implementation avaialble at [4].\n",
      " test_output.seq2seq.tsv is from our re-implementation of [5].\n",
      " test_output.namas.tsv is from the open-source implementation [6] of [7]. \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " References\n",
      " \n",
      " 1. Kristina Toutanova, Chris Brockett, Ke M. Tran, and Saleema Amershi. A dataset and evaluation metrics for abstractive sentence and paragraph compression. In Proceedings of EMNLP 2016.\n",
      " 2. Trevor Cohn and Mirella Lapata. Sentence compression beyond word deletion. In Proceedings of COLING 2008.\n",
      " 3. James Clarke and Mirella Lapata. Global inference for sentence compression: An integer linear programming approach. JAIR 2008.\n",
      " 4. https://github.com/cnap/sentence-compression\n",
      " 5. Katja Filippova, Enrique Alfonseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals. Sentence  compression  by  deletion  with  LSTMs. In Proceedings of EMNLP 2015.\n",
      " 6. https://github.com/facebook/NAMAS\n",
      " 7. Alexander M. Rush, Sumit Chopra, and  Jason  Weston.A  neural attention model for abstractive sentence summarization. In Proceedings of EMNLP 2015.\n",
      " \n",
      " \n",
      "\n",
      "Original: 30988\n",
      "Compress: 36565\n"
     ]
    }
   ],
   "source": [
    "#LZW Compression\n",
    "import os\n",
    "\n",
    "def compress(uncompressed):\n",
    "    \"\"\"Compress a string to a list of output symbols.\"\"\"\n",
    " \n",
    "    # Build the dictionary.\n",
    "    dict_size = 256\n",
    "    dictionary = dict((chr(i), i) for i in xrange(dict_size))\n",
    "    # in Python 3: dictionary = {chr(i): i for i in range(dict_size)}\n",
    " \n",
    "    w = \"\"\n",
    "    result = []\n",
    "    for c in uncompressed:\n",
    "        wc = w + c\n",
    "        if wc in dictionary:\n",
    "            w = wc\n",
    "        else:\n",
    "            result.append(dictionary[w])\n",
    "            # Add wc to the dictionary.\n",
    "            dictionary[wc] = dict_size\n",
    "            dict_size += 1\n",
    "            w = c\n",
    " \n",
    "    # Output the code for w.\n",
    "    if w:\n",
    "        result.append(dictionary[w])\n",
    "    return result\n",
    " \n",
    " \n",
    "def decompress(compressed):\n",
    "    \"\"\"Decompress a list of output ks to a string.\"\"\"\n",
    "    from cStringIO import StringIO\n",
    " \n",
    "    # Build the dictionary.\n",
    "    dict_size = 256\n",
    "    dictionary = dict((i, chr(i)) for i in xrange(dict_size))\n",
    "    # in Python 3: dictionary = {i: chr(i) for i in range(dict_size)}\n",
    " \n",
    "    # use StringIO, otherwise this becomes O(N^2)\n",
    "    # due to string concatenation in a loop\n",
    "    result = StringIO()\n",
    "    w = chr(compressed.pop(0))\n",
    "    result.write(w)\n",
    "    for k in compressed:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == dict_size:\n",
    "            entry = w + w[0]\n",
    "        else:\n",
    "            raise ValueError('Bad compressed k: %s' % k)\n",
    "        result.write(entry)\n",
    " \n",
    "        # Add w+entry[0] to the dictionary.\n",
    "        dictionary[dict_size] = w + entry[0]\n",
    "        dict_size += 1\n",
    " \n",
    "        w = entry\n",
    "    return result.getvalue()\n",
    " \n",
    "def save_to_file(path,thelist):\n",
    "    with open(path,\"wt\") as thefile:\n",
    "        for item in thelist:\n",
    "            thefile.write(\"%d`\" % item)\n",
    "        thefile.seek(-1, os.SEEK_END)\n",
    "        thefile.truncate()\n",
    "    thefile.close()\n",
    "    return\n",
    "\n",
    "def read_from_file(path):\n",
    "    with open(path, \"r\") as ins:\n",
    "        array = []\n",
    "        for line in ins.read().split('`'):\n",
    "            array.append(int(line))\n",
    "    ins.close()\n",
    "    return array\n",
    "\n",
    "def runLZW(self):\n",
    "    path = os.getcwd()+self\n",
    "    result_file = open(path+\"\\\\result\\\\resultLZW.txt\",\"wt\")\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(\"\\\\\"+filename+\": \")\n",
    "            test_file = open(path+\"\\\\\"+filename,\"r\")\n",
    "            write_file = path+\"\\compress\\compressLZW_\"+filename\n",
    "            \n",
    "            text = test_file.read()\n",
    "            a=compress(text)\n",
    "            save_to_file(write_file,a)\n",
    "            c=read_from_file(write_file)\n",
    "            b=decompress(c)\n",
    "            \n",
    "            save_file = path+\"\\decompress\\decompressLZW_\"+filename\n",
    "            decompress_file = open(save_file,\"wt\")\n",
    "            decompress_file.write(b)\n",
    "            decompress_file.close()\n",
    "            test_file.close()\n",
    "            \n",
    "            compress_size=os.path.getsize(write_file)\n",
    "            decompress_size=os.path.getsize(save_file)\n",
    "            print(b)\n",
    "            print(\"Original: %d\"%decompress_size)\n",
    "            print(\"Compress: %d\"%compress_size)\n",
    "            result_file.write(str(filename)+\"\\t\"+str(decompress_size)+\"\\t\"+str(compress_size)+\"\\n\")\n",
    "            \n",
    "    result_file.close()                   \n",
    "    return \n",
    "\n",
    "self = \"\\dataset\\\\a\"\n",
    "runLZW(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\001.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 117273\n",
      "Compress: 41024\n",
      "\\002.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 6835199\n",
      "Compress: 1507201\n",
      "\\003.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 1367999\n",
      "Compress: 720001\n",
      "\\004.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 30980\n",
      "Compress: 17817\n",
      "\\005.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 2041506\n",
      "Compress: 593256\n",
      "\\006.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 4079\n",
      "Compress: 2284\n",
      "\\007.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 3410\n",
      "Compress: 1924\n",
      "\\008.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 7480\n",
      "Compress: 4519\n",
      "\\009.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 3772\n",
      "Compress: 2244\n",
      "\\010.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 4833\n",
      "Compress: 2929\n",
      "\\011.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 2881\n",
      "Compress: 1764\n",
      "\\012.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 21983\n",
      "Compress: 12056\n",
      "\\013.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 24344\n",
      "Compress: 13535\n",
      "\\014.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 55895\n",
      "Compress: 30710\n",
      "\\015.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 90382\n",
      "Compress: 49639\n",
      "\\016.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 135279\n",
      "Compress: 74265\n",
      "\\017.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 221241\n",
      "Compress: 121077\n",
      "\\018.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 391593\n",
      "Compress: 215332\n",
      "\\019.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 940743\n",
      "Compress: 516884\n",
      "\\020.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 10554\n",
      "Compress: 6366\n",
      "\\021.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 1859258\n",
      "Compress: 1033811\n",
      "\\022.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 184184\n",
      "Compress: 111643\n",
      "\\023.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 264679\n",
      "Compress: 165336\n",
      "\\024.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 33008\n",
      "Compress: 19283\n",
      "\\025.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 788086\n",
      "Compress: 421270\n",
      "\\026.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 710752\n",
      "Compress: 444998\n",
      "\\027.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 230591\n",
      "Compress: 149650\n",
      "\\028.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 241313\n",
      "Compress: 146154\n",
      "\\029.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 4422547\n",
      "Compress: 2229211\n",
      "\\030.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 1887517\n",
      "Compress: 1080349\n",
      "\\031.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 4152592\n",
      "Compress: 2862466\n",
      "\\032.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 7556128\n",
      "Compress: 4895061\n",
      "\\033.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 7414257\n",
      "Compress: 4231844\n",
      "\\034.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 5290126\n",
      "Compress: 2969546\n",
      "\\035.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 5441761\n",
      "Compress: 3047266\n",
      "\\036.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 5374893\n",
      "Compress: 3003644\n",
      "\\037.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 5003088\n",
      "Compress: 2803731\n",
      "\\038.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 110592\n",
      "Compress: 13826\n",
      "\\039.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 388080\n",
      "Compress: 48512\n",
      "\\040.txt: \n",
      "Compressed\n",
      "Decompressed\n",
      "Original: 359948\n",
      "Compress: 224453\n"
     ]
    }
   ],
   "source": [
    "#HUFFMAN CODING\n",
    "import heapq\n",
    "import os\n",
    "import shutil\n",
    "from functools import total_ordering\n",
    "\n",
    "\n",
    "@total_ordering\n",
    "class HeapNode:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    # defining comparators less_than and equals\n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if(other == None):\n",
    "            return False\n",
    "        if(not isinstance(other, HeapNode)):\n",
    "            return False\n",
    "        return self.freq == other.freq\n",
    "\n",
    "\n",
    "class HuffmanCoding:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.heap = []\n",
    "        self.codes = {}\n",
    "        self.reverse_mapping = {}\n",
    "\n",
    "    # functions for compression:\n",
    "\n",
    "    def make_frequency_dict(self, text):\n",
    "        frequency = {}\n",
    "        for character in text:\n",
    "            if not character in frequency:\n",
    "                frequency[character] = 0\n",
    "            frequency[character] += 1\n",
    "        return frequency\n",
    "\n",
    "    def make_heap(self, frequency):\n",
    "        for key in frequency:\n",
    "            node = HeapNode(key, frequency[key])\n",
    "            heapq.heappush(self.heap, node)\n",
    "\n",
    "    def merge_nodes(self):\n",
    "        while(len(self.heap)>1):\n",
    "            node1 = heapq.heappop(self.heap)\n",
    "            node2 = heapq.heappop(self.heap)\n",
    "\n",
    "            merged = HeapNode(None, node1.freq + node2.freq)\n",
    "            merged.left = node1\n",
    "            merged.right = node2\n",
    "\n",
    "            heapq.heappush(self.heap, merged)\n",
    "\n",
    "\n",
    "    def make_codes_helper(self, root, current_code):\n",
    "        if(root == None):\n",
    "            return\n",
    "\n",
    "        if(root.char != None):\n",
    "            self.codes[root.char] = current_code\n",
    "            self.reverse_mapping[current_code] = root.char\n",
    "            return\n",
    "\n",
    "        self.make_codes_helper(root.left, current_code + \"0\")\n",
    "        self.make_codes_helper(root.right, current_code + \"1\")\n",
    "\n",
    "\n",
    "    def make_codes(self):\n",
    "        root = heapq.heappop(self.heap)\n",
    "        current_code = \"\"\n",
    "        self.make_codes_helper(root, current_code)\n",
    "\n",
    "\n",
    "    def get_encoded_text(self, text):\n",
    "        encoded_text = \"\"\n",
    "        for character in text:\n",
    "            encoded_text += self.codes[character]\n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "    def pad_encoded_text(self, encoded_text):\n",
    "        extra_padding = 8 - len(encoded_text) % 8\n",
    "        for i in range(extra_padding):\n",
    "            encoded_text += \"0\"\n",
    "\n",
    "        padded_info = \"{0:08b}\".format(extra_padding)\n",
    "        encoded_text = padded_info + encoded_text\n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "    def get_byte_array(self, padded_encoded_text):\n",
    "        if(len(padded_encoded_text) % 8 != 0):\n",
    "            print(\"Encoded text not padded properly\")\n",
    "            exit(0)\n",
    "\n",
    "        b = bytearray()\n",
    "        for i in range(0, len(padded_encoded_text), 8):\n",
    "            byte = padded_encoded_text[i:i+8]\n",
    "            b.append(int(byte, 2))\n",
    "        return b\n",
    "\n",
    "\n",
    "    def compress(self):\n",
    "        filename, file_extension = os.path.splitext(self.path)\n",
    "        output_path = filename + \".bin\"\n",
    "\n",
    "        with open(self.path, 'r+') as file, open(output_path, 'wb') as output:\n",
    "            text = file.read()\n",
    "            text = text.rstrip()\n",
    "\n",
    "            frequency = self.make_frequency_dict(text)\n",
    "            self.make_heap(frequency)\n",
    "            self.merge_nodes()\n",
    "            self.make_codes()\n",
    "\n",
    "            encoded_text = self.get_encoded_text(text)\n",
    "            padded_encoded_text = self.pad_encoded_text(encoded_text)\n",
    "\n",
    "            b = self.get_byte_array(padded_encoded_text)\n",
    "            output.write(bytes(b))\n",
    "\n",
    "        print(\"Compressed\")\n",
    "        return output_path\n",
    "\n",
    "\n",
    "    \"\"\" functions for decompression: \"\"\"\n",
    "\n",
    "\n",
    "    def remove_padding(self, padded_encoded_text):\n",
    "        padded_info = padded_encoded_text[:8]\n",
    "        extra_padding = int(padded_info, 2)\n",
    "\n",
    "        padded_encoded_text = padded_encoded_text[8:] \n",
    "        encoded_text = padded_encoded_text[:-1*extra_padding]\n",
    "\n",
    "        return encoded_text\n",
    "\n",
    "    def decode_text(self, encoded_text):\n",
    "        current_code = \"\"\n",
    "        decoded_text = \"\"\n",
    "\n",
    "        for bit in encoded_text:\n",
    "            current_code += bit\n",
    "            if(current_code in self.reverse_mapping):\n",
    "                character = self.reverse_mapping[current_code]\n",
    "                decoded_text += character\n",
    "                current_code = \"\"\n",
    "\n",
    "        return decoded_text\n",
    "\n",
    "\n",
    "    def decompress(self, input_path):\n",
    "        filename, file_extension = os.path.splitext(self.path)\n",
    "        output_path = filename + \"_decompressed\" + \".txt\"\n",
    "\n",
    "        with open(input_path, 'rb') as file, open(output_path, 'w') as output:\n",
    "            bit_string = \"\"\n",
    "\n",
    "            byte = file.read(1)\n",
    "            while(len(byte) > 0):\n",
    "                byte = ord(byte)\n",
    "                bits = bin(byte)[2:].rjust(8, '0')\n",
    "                bit_string += bits\n",
    "                byte = file.read(1)\n",
    "\n",
    "            encoded_text = self.remove_padding(bit_string)\n",
    "\n",
    "            decompressed_text = self.decode_text(encoded_text)\n",
    "            output.write(decompressed_text)\n",
    "\n",
    "        print(\"Decompressed\")\n",
    "        return output_path\n",
    "\n",
    "def runHuffman(self):\n",
    "    path = os.getcwd()+self\n",
    "    result_file = open(path+\"\\\\result\\\\resultHuffman.txt\",\"wt\")\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(\"\\\\\"+filename+\": \")\n",
    "            test_file = path+\"\\\\\"+filename\n",
    "            h = HuffmanCoding(test_file)\n",
    "            output_path = h.compress()\n",
    "            write_file = path+\"\\compress\\compressHuffman_\"+filename\n",
    "            write_file=write_file.replace(\".txt\",\".bin\")\n",
    "            shutil.move(output_path, write_file)\n",
    "            \n",
    "            decom_path = h.decompress(write_file)\n",
    "            save_file = path+\"\\decompress\\decompressHuffman_\"+filename\n",
    "            shutil.move(decom_path, save_file)\n",
    "\n",
    "            \n",
    "            compress_size=os.path.getsize(write_file)\n",
    "            decompress_size=os.path.getsize(save_file)\n",
    "            \n",
    "            print(\"Original: %d\"%decompress_size)\n",
    "            print(\"Compress: %d\"%compress_size)\n",
    "            result_file.write(str(filename)+\"\\t\"+str(decompress_size)+\"\\t\"+str(compress_size)+\"\\n\")\n",
    "            \n",
    "    result_file.close()                   \n",
    "    return \n",
    "\n",
    "self = \"\\dataset\"\n",
    "runHuffman(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "main() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e6a1a662e710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-e\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"saasdasdasds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: main() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
